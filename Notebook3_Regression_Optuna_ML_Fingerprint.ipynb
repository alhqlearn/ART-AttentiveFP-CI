{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e669c2c1-0aa0-472d-9106-db2aaa4c4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, Matern, RationalQuadratic\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import torch.multiprocessing as mp\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from tqdm import tqdm\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import os\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0137d46e-0a4e-4fa8-b245-4d7d2aa77797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('ART_main.csv',encoding='ISO-8859-1')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ab459-ae1e-4728-a324-8910775ab1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=np.array(df['ee'])\n",
    "mols=[Chem.MolFromSmiles(i) for i in df['smiles']]\n",
    "features=[AllChem.GetMorganFingerprintAsBitVect(i, 2, nBits=1024) for i in mols]\n",
    "features1=[rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=1024) for mol in mols]\n",
    "features2=[Chem.LayeredFingerprint(mol, fpSize=1024) for mol in mols]\n",
    "\n",
    "fps1= np.array(features)\n",
    "fps2= np.array(features1)\n",
    "fps3= np.array(features2)\n",
    "\n",
    "concatenated_fps = np.concatenate((fps1, fps2, fps3), axis=1)\n",
    "concatenated_fps_df = pd.DataFrame(concatenated_fps)\n",
    "concatenated_fps_df['ee'] = outputs\n",
    "print(concatenated_fps_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc906b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(concatenated_fps_df.iloc[:,:-1])\n",
    "y=np.array(concatenated_fps_df['ee']).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6880d89-1e6d-4710-9cb1-57c9d87119eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset and preprocess it\n",
    "def dataset_func(X, y, rand_state):\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_state)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.12, random_state=rand_state)\n",
    "\n",
    "    print(\"Train Dataset: {}\".format(X_train.shape))\n",
    "    print(\"Val Dataset: {}\".format(X_val.shape))\n",
    "    print(\"Test Dataset: {}\".format(X_test.shape))\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ae69f-4e0c-41ca-b4dc-c3aaa6227362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset and preprocess i\n",
    "\n",
    "# Function to define and train the Random Forest Regressor model\n",
    "def train_random_forest(X_train, y_train, n_estimators, max_depth, rand_state):\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=rand_state)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Function to define and train the Random Forest Regressor model\n",
    "def train_decision_tree(X_train, y_train, max_depth, rand_state):\n",
    "    model = DecisionTreeRegressor(max_depth=max_depth, random_state=rand_state)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to define and train the Gradient Boosting Regressor model\n",
    "def train_gradient_boosting(X_train, y_train, n_estimators, max_depth, rand_state):\n",
    "    model = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=rand_state)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "class SVMPredictor:\n",
    "    def __init__(self, svm_callable):\n",
    "        self.svm_callable = svm_callable\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.svm_callable(X)\n",
    "\n",
    "# Modify train_svm to return an instance of SVMPredictor\n",
    "def train_svm(X_train, y_train, C, epsilon, kernel):\n",
    "    model = SVR(C=C, epsilon=epsilon, kernel=kernel)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Return an instance of SVMPredictor\n",
    "    return SVMPredictor(lambda x: model.predict(x))\n",
    "\n",
    "\n",
    "# Function to evaluate the model on train, validation, and test sets\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    train_predictions = model.predict(X_train)\n",
    "    val_predictions = model.predict(X_val)\n",
    "    test_predictions = model.predict(X_test)\n",
    "\n",
    "    train_rmse = math.sqrt(mean_squared_error(y_train, train_predictions))\n",
    "    val_rmse = math.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "    test_rmse = math.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    val_r2 = r2_score(y_val, val_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "    return train_rmse, val_rmse, test_rmse, train_r2, val_r2, test_r2\n",
    "\n",
    "def plot_permutation_importance(model, X, y, save_path):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        feature_names = X.columns\n",
    "    elif isinstance(X, np.ndarray):\n",
    "        feature_names = np.arange(X.shape[1])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input type. Use either Pandas DataFrame or NumPy array.\")\n",
    "\n",
    "    result = permutation_importance(model, X, y, n_repeats=30, random_state=0)\n",
    "    sorted_idx = result.importances_mean.argsort()\n",
    "    \n",
    "\n",
    "    plt.barh(range(20), result.importances_mean[sorted_idx[-20:]])\n",
    "    plt.yticks(range(20), feature_names[sorted_idx[0:20]])\n",
    "    plt.xlabel('Permutation Importance')\n",
    "    plt.title('Permutation Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "def plot_shap_importance(model, X, save_path):\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "    shap.summary_plot(shap_values, X, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "def plot_shap_importance_svm_gpr(model, X_tr, X_val, save_path):\n",
    "    # Create a Kernel SHAP explainer\n",
    "    explainer = shap.Explainer(model.predict, X_tr)\n",
    "    \n",
    "    # Compute SHAP values for the test set\n",
    "    shap_values = explainer.shap_values(X_val)\n",
    "    \n",
    "    shap.summary_plot(shap_values, X_val, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "def objective_rf(trial, X_train, y_train, X_val, y_val):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 1,100)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 100)\n",
    "    model = train_random_forest(X_train, y_train, n_estimators, max_depth, rand_state)\n",
    "    val_preds = model.predict(X_val)\n",
    "    val_loss = mean_squared_error(y_val, val_preds)\n",
    "    train_preds = model.predict(X_train)\n",
    "    train_loss = mean_squared_error(y_train, train_preds)\n",
    "    obj_val = val_loss-train_loss\n",
    "    #print('train_loss:',train_loss)\n",
    "    #print('val_loss:',val_loss)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def objective_dt(trial, X_train, y_train, X_val, y_val):\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 100)\n",
    "    model = train_decision_tree(X_train, y_train, max_depth, rand_state)\n",
    "    val_preds = model.predict(X_val)\n",
    "    val_loss = mean_squared_error(y_val, val_preds)\n",
    "    return val_loss\n",
    "    \n",
    "\n",
    "def objective_gb(trial, X_train, y_train, X_val, y_val):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 1, 100)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 100)\n",
    "    model = train_gradient_boosting(X_train, y_train, n_estimators, max_depth, rand_state)\n",
    "    val_preds = model.predict(X_val)\n",
    "    val_loss = mean_squared_error(y_val, val_preds)\n",
    "    return val_loss\n",
    "\n",
    "def objective_svm(trial, X_train, y_train, X_val, y_val):\n",
    "    C = trial.suggest_loguniform('C', 1, 30)\n",
    "    epsilon = trial.suggest_loguniform('epsilon', 1e-5, 1e5)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
    "    model = train_svm(X_train, y_train, C, epsilon, kernel)\n",
    "    val_preds = model.predict(X_val)\n",
    "    val_loss = mean_squared_error(y_val, val_preds)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def objective_func_for_model(model_name,X_train, y_train, X_val, y_val,trial):\n",
    "    if model_name == 'Random Forest':\n",
    "        return objective_rf(trial, X_train, y_train, X_val, y_val)\n",
    "    elif model_name == 'Decision tree':\n",
    "        return objective_dt(trial, X_train, y_train, X_val, y_val)\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        return objective_gb(trial, X_train, y_train, X_val, y_val)\n",
    "    elif model_name == 'SVM':\n",
    "        return objective_svm(trial, X_train, y_train, X_val, y_val)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "def optimize_model(params,save_path_permutation, save_path_shap):\n",
    "    seed, model_name, objective_func, n_trials, X, y = params\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = dataset_func(X, y, seed)\n",
    "    \n",
    "    # Modify the partial function to pass additional arguments\n",
    "    objective_func_partial = partial(objective_func_for_model, model_name, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    study.optimize(objective_func_partial, n_trials)\n",
    "    best_params = study.best_params\n",
    "    final_model = None\n",
    "\n",
    "    if model_name == 'Random Forest':\n",
    "        final_model = train_random_forest(X_train, y_train, best_params['n_estimators'], best_params['max_depth'], seed)\n",
    "        #plot_permutation_importance(final_model, X_val, y_val, save_path_permutation)\n",
    "        #plot_shap_importance(final_model, X_val, save_path_shap)\n",
    "    elif model_name == 'Decision tree':\n",
    "        final_model = train_decision_tree(X_train, y_train, best_params['max_depth'], seed)\n",
    "        #plot_permutation_importance(final_model, X_val, y_val, save_path_permutation)\n",
    "        #plot_shap_importance(final_model, X_val, save_path_shap)\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        final_model = train_gradient_boosting(X_train, y_train, best_params['n_estimators'], best_params['max_depth'], seed)\n",
    "        #plot_permutation_importance(final_model, X_val, y_val, save_path_permutation)\n",
    "        #plot_shap_importance(final_model, X_val, save_path_shap)\n",
    "    elif model_name == 'SVM':\n",
    "        final_model = train_svm(X_train, y_train, best_params['C'], best_params['epsilon'], best_params['kernel'])\n",
    "        #plot_shap_importance_svm_gpr(final_model, X_train, X_val, save_path_shap)\n",
    "\n",
    "    train_rmse, val_rmse, test_rmse, train_r2, val_r2, test_r2 = evaluate_model(final_model, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "    return {'Model': model_name, 'Seed': seed,\n",
    "            'Train RMSE': train_rmse, 'Val RMSE': val_rmse, 'Test RMSE': test_rmse,\n",
    "            'Train R2': train_r2, 'Val R2': val_r2, 'Test R2': test_r2, 'Best Parameters': best_params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05ba85-7302-4a90-b932-7403f6b6a6f9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set random state\n",
    "rand_states = [i for i in range(0,30)]  # Add more seed values as needed\n",
    "\n",
    "# Create an empty list to store futures\n",
    "futures = []\n",
    "\n",
    "# Use a ProcessPoolExecutor for parallel processing\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    for model_name in ['Random Forest','Decision tree', 'Gradient Boosting','SVM',]: # , 'Random Forest','GPR' , 'Gradient Boosting','SVM'\n",
    "        for rand_state in rand_states:\n",
    "            # Pass parameters as a tuple\n",
    "            params = (rand_state, model_name, objective_func_for_model, 100, X, y)\n",
    "            save_path_permutation = f'{model_name}_seed_{rand_state}_permutation_importance_plot.png'\n",
    "            save_path_shap = f'{model_name}_seed_{rand_state}_shap_importance_plot.png'\n",
    "            # Submit each optimization task as a future\n",
    "            futures.append(executor.submit(optimize_model, params, save_path_permutation, save_path_shap))\n",
    "\n",
    "# Collect the results from completed futures\n",
    "results_list = [future.result() for future in futures]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af068e-13d0-4b59-b4e9-b001274783ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "results_df_rounded = results_df.round(3)\n",
    "results_df_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5896a-4d53-4c26-9613-0a7c342411be",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_rounded.to_csv('performance_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a606c-5504-41e9-ba7f-f1a85a62c7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn_suzuki",
   "language": "python",
   "name": "dnn_suzuki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
