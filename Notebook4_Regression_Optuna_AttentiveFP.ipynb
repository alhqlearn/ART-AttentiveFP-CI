{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a08f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import optuna\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from tqdm import tqdm\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import os \n",
    "import sys\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, rdMolDescriptors, MACCSkeys\n",
    "from rdkit.Avalon import pyAvalonTools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c268bd-4494-4ac4-a62b-2825dee9c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dgllife.model import model_zoo\n",
    "from dgllife.utils import smiles_to_bigraph\n",
    "from dgllife.utils import EarlyStopping, Meter\n",
    "from dgllife.utils import AttentiveFPAtomFeaturizer\n",
    "from dgllife.utils import AttentiveFPBondFeaturizer\n",
    "from dgllife.data import MoleculeCSVDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f16a9-d8d3-496a-909b-d51ec8ffd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a24f7-0aa0-4a49-b000-6bda0639bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_molgraphs(data):\n",
    "    assert len(data[0]) in [3, 4], \\\n",
    "        'Expect the tuple to be of length 3 or 4, got {:d}'.format(len(data[0]))\n",
    "    if len(data[0]) == 3:\n",
    "        smiles, graphs, labels = map(list, zip(*data))\n",
    "        masks = None\n",
    "    else:\n",
    "        smiles, graphs, labels, masks = map(list, zip(*data))\n",
    "\n",
    "    bg = dgl.batch(graphs)\n",
    "    bg.set_n_initializer(dgl.init.zero_initializer)\n",
    "    bg.set_e_initializer(dgl.init.zero_initializer)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "\n",
    "    if masks is None:\n",
    "        masks = torch.ones(labels.shape)\n",
    "    else:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "    return smiles, bg, labels, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, prediction, labels, masks, loss_criterion):\n",
    "    mse_loss = (loss_criterion(prediction, labels) * (masks != 0).float()).mean()\n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592554c-8cd3-4954-a1a5-9410dc17c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(train_loader, valid_loader, test_loader, num_epoch):\n",
    "    def objective_inner(trial):\n",
    "        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "        graph_feat_size = trial.suggest_int('graph_feat_size', 100, 500)\n",
    "        dropout_rate = round(trial.suggest_uniform('dropout_rate', 0.0, 0.5), 5)\n",
    "        learning_rate = round(trial.suggest_loguniform('learning_rate', 1e-5, 1e-1), 5)\n",
    "        num_timesteps = trial.suggest_int('num_timesteps', 1, 3)\n",
    "        \n",
    "        atom_featurizer = AttentiveFPAtomFeaturizer(atom_data_field='hv')\n",
    "        bond_featurizer = AttentiveFPBondFeaturizer(bond_data_field='he')\n",
    "        n_feats = atom_featurizer.feat_size('hv')\n",
    "        e_feats = bond_featurizer.feat_size('he')\n",
    "\n",
    "        model = model_zoo.AttentiveFPPredictor(node_feat_size=n_feats,\n",
    "                                           edge_feat_size=e_feats,\n",
    "                                           num_layers=num_layers,\n",
    "                                           num_timesteps=num_timesteps,\n",
    "                                           graph_feat_size=graph_feat_size,\n",
    "                                           n_tasks=1,\n",
    "                                           dropout=dropout_rate\n",
    "                                            )\n",
    "        model = model.to(device)\n",
    "        #Train\n",
    "        loss_criterion = nn.MSELoss(reduction='none')\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)    \n",
    "            \n",
    "        for epoch in range(num_epoch):\n",
    "            model.train()\n",
    "            losses = []\n",
    "            train_meter = Meter()\n",
    "            for batch_id, batch_data in enumerate(train_loader):\n",
    "                smiles, bg, labels, masks = batch_data\n",
    "                bg=bg.to(device)\n",
    "                labels = labels.to(device)\n",
    "                masks = masks.to(device)\n",
    "                n_feats = bg.ndata.pop('hv').to(device)\n",
    "                e_feats = bg.edata.pop('he').to(device)\n",
    "                prediction = model(bg, n_feats, e_feats)\n",
    "                #loss = (loss_criterion(prediction, labels) * (masks != 0).float()).mean()\n",
    "                loss = compute_loss(model, prediction, labels, masks, loss_criterion)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.data.item())\n",
    "            total_train_loss = np.mean(losses)\n",
    "    \n",
    "        model.eval()\n",
    "        val_losses=[]\n",
    "        eval_meter = Meter()\n",
    "        with torch.no_grad():\n",
    "            for batch_id, batch_data in enumerate(valid_loader):\n",
    "                smiles, bg, labels, masks = batch_data\n",
    "                bg = bg.to(device)\n",
    "                labels = labels.to(device)\n",
    "                masks = masks.to(device)\n",
    "                n_feats = bg.ndata.pop('hv').to(device)\n",
    "                e_feats = bg.edata.pop('he').to(device)\n",
    "                valid_prediction = model(bg, n_feats, e_feats)\n",
    "                #val_loss = (loss_criterion(valid_prediction, labels) * (masks != 0).float()).mean()\n",
    "                val_loss = compute_loss(model, valid_prediction, labels, masks, loss_criterion)\n",
    "                val_loss=val_loss.detach().cpu().numpy()\n",
    "                val_losses.append(val_loss)\n",
    "            total_val_loss = np.mean(val_losses)\n",
    "        return total_val_loss\n",
    "    return objective_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340ff86-4503-49fb-aa6d-32e8da2b0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(best_params, num_epoch, train_loader, valid_loader, test_loader, cv_name):\n",
    "    device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_n_layers = best_params['num_layers']\n",
    "    best_graph_feat_size = best_params['graph_feat_size']\n",
    "    best_dropout_rate = best_params['dropout_rate']\n",
    "    best_learning_rate = best_params['learning_rate']\n",
    "    best_num_timesteps = best_params['num_timesteps']\n",
    "\n",
    "    atom_featurizer = AttentiveFPAtomFeaturizer(atom_data_field='hv')\n",
    "    bond_featurizer = AttentiveFPBondFeaturizer(bond_data_field='he')\n",
    "    n_feats = atom_featurizer.feat_size('hv')\n",
    "    e_feats = bond_featurizer.feat_size('he')\n",
    "\n",
    "    model = model_zoo.AttentiveFPPredictor(node_feat_size=n_feats,\n",
    "                                       edge_feat_size=e_feats,\n",
    "                                       num_layers=best_n_layers,\n",
    "                                       num_timesteps=best_num_timesteps,\n",
    "                                       graph_feat_size=best_graph_feat_size,\n",
    "                                       n_tasks=1,\n",
    "                                       dropout=best_dropout_rate\n",
    "                                        )\n",
    "    model = model.to(device)\n",
    "    #Train\n",
    "    loss_criterion = nn.MSELoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_learning_rate) \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for batch_id, batch_data in enumerate(train_loader):\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            bg=bg.to(device)\n",
    "            labels = labels.to(device)\n",
    "            masks = masks.to(device)\n",
    "            n_feats = bg.ndata.pop('hv').to(device)\n",
    "            e_feats = bg.edata.pop('he').to(device)\n",
    "            prediction = model(bg, n_feats, e_feats)\n",
    "            #loss = (loss_criterion(prediction, labels) * (masks != 0).float()).mean()\n",
    "            loss = compute_loss(model, prediction, labels, masks, loss_criterion)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.data.item())\n",
    "        total_train_loss = np.mean(losses)\n",
    "        train_loss_history.append(total_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_losses=[]\n",
    "        with torch.no_grad():\n",
    "            for batch_id, batch_data in enumerate(valid_loader):\n",
    "                smiles, bg, labels, masks = batch_data\n",
    "                bg = bg.to(device)\n",
    "                labels = labels.to(device)\n",
    "                masks = masks.to(device)\n",
    "                n_feats = bg.ndata.pop('hv').to(device)\n",
    "                e_feats = bg.edata.pop('he').to(device)\n",
    "                valid_prediction = model(bg, n_feats, e_feats)\n",
    "                val_loss = (loss_criterion(valid_prediction, labels) * (masks != 0).float()).mean()\n",
    "                val_loss=val_loss.detach().cpu().numpy()\n",
    "                val_losses.append(val_loss)\n",
    "            total_val_loss = np.mean(val_losses)\n",
    "            val_loss_history.append(total_val_loss)\n",
    "\n",
    "    num_epoch_plot = num_epoch + 1\n",
    "    plt.plot(range(1, num_epoch_plot), train_loss_history, label='Train Loss')\n",
    "    plt.plot(range(1, num_epoch_plot), val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #model_path = 'model_attentive_fp_' + cv_name + '.pth'\n",
    "    #torch.save(model.state_dict(), model_path)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        labels_app = []\n",
    "        train_prediction_app = []\n",
    "        for batch_id, batch_data in enumerate(train_loader):\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            bg = bg.to(device)\n",
    "            labels = labels.to(device)\n",
    "            masks = masks.to(device)\n",
    "            n_feats = bg.ndata.pop('hv').to(device)\n",
    "            e_feats = bg.edata.pop('he').to(device)\n",
    "            train_prediction = model(bg, n_feats, e_feats)\n",
    "            labels_app.extend(labels)\n",
    "            train_prediction_app.extend(train_prediction)\n",
    "        train_rmse = math.sqrt(criterion(torch.cat(train_prediction_app), torch.cat(labels_app)).item())\n",
    "        train_r2 = r2_score(torch.cat(labels_app).detach().cpu().numpy(), torch.cat(train_prediction_app).detach().cpu().numpy())\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        labels_app = []\n",
    "        valid_prediction_app = []\n",
    "        for batch_id, batch_data in enumerate(valid_loader):\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            bg = bg.to(device)\n",
    "            labels = labels.to(device)\n",
    "            masks = masks.to(device)\n",
    "            n_feats = bg.ndata.pop('hv').to(device)\n",
    "            e_feats = bg.edata.pop('he').to(device)\n",
    "            valid_prediction = model(bg, n_feats, e_feats)\n",
    "            labels_app.extend(labels)\n",
    "            valid_prediction_app.extend(valid_prediction)\n",
    "        valid_rmse = math.sqrt(criterion(torch.cat(valid_prediction_app), torch.cat(labels_app)).item())\n",
    "        valid_r2 = r2_score(torch.cat(labels_app).detach().cpu().numpy(), torch.cat(valid_prediction_app).detach().cpu().numpy())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        labels_app = []\n",
    "        test_prediction_app = []\n",
    "        for batch_id, batch_data in enumerate(test_loader):\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            bg = bg.to(device)\n",
    "            labels = labels.to(device)\n",
    "            masks = masks.to(device)\n",
    "            n_feats = bg.ndata.pop('hv').to(device)\n",
    "            e_feats = bg.edata.pop('he').to(device)\n",
    "            test_prediction = model(bg, n_feats, e_feats)\n",
    "            labels_app.extend(labels)\n",
    "            test_prediction_app.extend(test_prediction)\n",
    "        test_rmse = math.sqrt(criterion(torch.cat(test_prediction_app), torch.cat(labels_app)).item())\n",
    "        test_r2 = r2_score(torch.cat(labels_app).detach().cpu().numpy(), torch.cat(test_prediction_app).detach().cpu().numpy())\n",
    "\n",
    "    return train_rmse, valid_rmse, test_rmse, train_r2, valid_r2, test_r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283b9b9-3a67-4210-8ef6-22979b991a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_featurizer = AttentiveFPAtomFeaturizer(atom_data_field='hv')\n",
    "bond_featurizer = AttentiveFPBondFeaturizer(bond_data_field='he')\n",
    "n_feats = atom_featurizer.feat_size('hv')\n",
    "e_feats = bond_featurizer.feat_size('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549b119-07cc-48a8-8dfe-c5ce6bf7ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_func(data_file):\n",
    "    X =data_file['smiles']\n",
    "    y=data_file['ee']\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "    print(\"Train Dataset: {}\".format(X_train.shape))\n",
    "    print(\"Val Dataset: {}\".format(X_val.shape))\n",
    "    print(\"Test Dataset: {}\".format(X_test.shape))\n",
    "\n",
    "    df_train = pd.concat([X_train, y_train], axis=1)\n",
    "    df_val = pd.concat([X_val, y_val], axis=1)\n",
    "    df_test = pd.concat([X_test, y_test], axis=1)\n",
    "    return df_train, df_val, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9231d-df23-4581-a9c5-c0905c366397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data,name):\n",
    "    cache_file_path = f\"{name}_dataset.bin\"\n",
    "    dataset = MoleculeCSVDataset(data,\n",
    "                                 smiles_to_graph=smiles_to_bigraph,\n",
    "                                 node_featurizer=atom_featurizer,\n",
    "                                 edge_featurizer= bond_featurizer,\n",
    "                                 smiles_column='smiles',\n",
    "                                 task_names=['ee'],init_mask=True,n_jobs=8,\n",
    "                                 cache_file_path=cache_file_path\n",
    "                                )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c44a51-19ee-412a-b362-aca272f77436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_fn(dc_listings1, dc_listings2, dc_listings3):\n",
    "    train_datasets = load_data(dc_listings1,'train')\n",
    "    valid_datasets = load_data(dc_listings2,'valid')\n",
    "    test_datasets = load_data(dc_listings3,'test')\n",
    "    train_loader = DataLoader(train_datasets, batch_size=32,shuffle=False,\n",
    "                              collate_fn=collate_molgraphs)\n",
    "    valid_loader = DataLoader(valid_datasets,batch_size=32,shuffle=False,\n",
    "                              collate_fn=collate_molgraphs)\n",
    "    test_loader = DataLoader(test_datasets,batch_size=32,shuffle=False,\n",
    "                              collate_fn=collate_molgraphs)\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45be5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = 'ART_30_splits.xlsx'\n",
    "CV_list_name = ['fullcv_00','fullcv_01','fullcv_02','fullcv_03','fullcv_04','fullcv_05','fullcv_06','fullcv_07','fullcv_08','fullcv_09',\n",
    "'fullcv_10','fullcv_11','fullcv_12','fullcv_13','fullcv_14','fullcv_15','fullcv_16','fullcv_17',\n",
    "'fullcv_18','fullcv_19','fullcv_20','fullcv_21','fullcv_22','fullcv_23','fullcv_24','fullcv_25',\n",
    "'fullcv_26','fullcv_27','fullcv_28','fullcv_29']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94754a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "best_params_app = []\n",
    "\n",
    "for i in CV_list_name:\n",
    "    num_epoch = 300\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    data_file = pd.read_excel(data_file_path, sheet_name=i)\n",
    "    dc_listings1, dc_listings2, dc_listings3 = dataset_func(data_file)\n",
    "    train_loader, valid_loader, test_loader = data_loader_fn(dc_listings1, dc_listings2, dc_listings3)\n",
    "    #print(next(iter(train_loader)))\n",
    "    \n",
    "    study.optimize(objective(train_loader, valid_loader, test_loader, num_epoch), n_trials=30)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_accuracy = study.best_value\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Accuracy:\", best_accuracy)\n",
    "\n",
    "    accuracy = get_all_metrics(best_params, num_epoch, train_loader, valid_loader, test_loader, i)\n",
    "    best_params_app.append(best_params)\n",
    "    metrics.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1125e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the train, valid, and test accuracy from the accuracy_list\n",
    "train_accuracy = [item[0] for item in metrics]\n",
    "valid_accuracy = [item[1] for item in metrics]\n",
    "test_accuracy = [item[2] for item in metrics]\n",
    "\n",
    "train_top_k = [item[3] for item in metrics]\n",
    "valid_top_k = [item[4] for item in metrics]\n",
    "test_top_k = [item[5] for item in metrics]\n",
    "\n",
    "# Create a dictionary from the accuracy values\n",
    "data_rs = {'Split': CV_list_name,\n",
    "        'Train RMSE': train_accuracy,\n",
    "        'Validation RMSE': valid_accuracy,\n",
    "        'Test RMSE': test_accuracy,\n",
    "        'Train_R2': train_top_k,\n",
    "        'Valid_R2': valid_top_k,\n",
    "        'Test_R2': test_top_k,\n",
    "        'Parameters': best_params_app}\n",
    "# Create a pandas DataFrame\n",
    "dff_result = pd.DataFrame(data_rs)\n",
    "dff_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed041b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_result.to_csv('Performance_file.csv')\n",
    "dff_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jak_mtatfp",
   "language": "python",
   "name": "jak_mtatfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
